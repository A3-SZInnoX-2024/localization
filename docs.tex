\documentclass{article}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{float}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{url}
\usepackage{lettrine}

\title{Navigation System of Group A2}
\author{Wu Chengyu\ \url{7086cmd@gmail.com}}
\date{\today}

\begin{document}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}
\lettrine{T}{he} map of the contest is a 2D plane with \texttt{apriltags} on it. The \texttt{apriltags} are asymmetrical and easy to recognize.

Through the camera, we can fetch all the \texttt{apriltags} in the view of the camera. Using \texttt{OpenCV}, we can get the position of the \texttt{apriltags} in the image. Then we can use the position of the \texttt{apriltags} to calculate the position of the camera.

The \texttt{ROS} (Robot Operating System) provides apis to get the position of the \texttt{apriltags} in the image. However, we also need the camera to recognize other blocks, e.g. the ``fish'' in the contest. So we deprecated the \texttt{ROS} and simply use \texttt{Python} with \texttt{OpenCV} to get the position of the \texttt{apriltags} in order to locate.

Also, we need to recognize blocks colored with red, yellow, green, and blue (size: $5\mathrm{cm}\times5\mathrm{cm}\times5\mathrm{cm}$). We should use \texttt{OpenCV} too, so it's a good manner to combine these two parts together.

That's the biggest reason why I strongly recommend navigating with \texttt{OpenCV}.

\section{Targets}

\subsection{Localization \& Navigation}

\begin{itemize}
  \item We need fetch the location of the robot (not only position, but also the orientation) in the ground coordinate system. The ground coordinate system is defined by the \texttt{apriltags}.
  \item We need to adjust the position, making it more accurate. The accuracy should be less than $1\mathrm{cm}$.
  \item We need to handle with the emergency situation. If there are less than $3$ \texttt{apriltags} in the view of the camera, we should use corner of single \& double \texttt{apriltags} to calculate the position of the camera.
  \item If there is no \texttt{apriltags} in the view of the camera, we should turn around and find the \texttt{apriltags}.
\end{itemize}

\subsection{Block Recognition}

\begin{itemize}
  \item We need to recognize the color of the block. The color of the block is red, yellow, green, and blue. If necessary, we need to recognize the orange block too.
  \item We should get the camera matrix, the distortion coefficients. However, it's not my task.
  \item We should also get the position of the block (only position) in the ground coordinate system.
\end{itemize}

\section{Methodological Analysis}
In general, positioning requires $6$ degrees of freedom. That is to say, we need at least $6$ points to calculate the accurate position of the camera. However, the $3$ degrees of freedom will remain constant during the robot's motion.

For spatial coordinates, we use the Cartesian coordinate system $\left(x,y,z\right)$. For the angular coordinates, we use the Euler angle $\left(\alpha,\beta,\gamma\right)$. Or we can call it $\left(\mathrm{roll},\mathrm{pitch},\mathrm{yaw}\right)$.

The punctuation of \texttt{apriltags} is fixed, therefore, as long as we know the position of \texttt{apriltags} in the image and its punctuation, it is possible to correspond the two-dimensional image coordinates to the three-dimensional spatial coordinates. Using P$n$P algorithm, we can get the position of the camera.

\subsection{Basic Parameters}
We need the camera's internal reference and distortion coefficients to accurately confirm the conversion.

The camera's internal reference is a $3\times3$ matrix, which is the camera's focal length and the center of the image:

\[
  \texttt{camera\_matrix}=\left(\begin{matrix}
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1
  \end{matrix}\right)
\]

The distortion coefficients are $5$ parameters, which are used to correct the distortion of the image:

\[
  \texttt{distortion\_coefficients}=\left(\begin{matrix}
    k_1 & k_2 & p_1 & p_2 & k_3
  \end{matrix}\right)
\]

The relevant parameters of the camera are not considered to vary excessively during the process. In other words, we can store these parameters directly at call time.

\subsection{6-point Adjustment}
Each location of the \texttt{apriltags} can define $\dfrac16$ of the camera's position. Therefore, we need at least $6$ \texttt{apriltags} to calculate the position of the camera.

\[
  \left(
    \begin{matrix}
      x&y&z\\
      \mathrm{roll}&\mathrm{pitch}&\mathrm{yaw}
    \end{matrix}
  \right)
\]

Through the P$n$P algorithm provided by \texttt{OpenCV}, we can calculate the transform vector and the rotation vector of the camera easily.

After getting \texttt{tvec} and \texttt{rvec}, we can use the \texttt{Rodrigues} function to convert the rotation vector to the rotation matrix:

\[
  \boldsymbol{R}\_\texttt{mtx}=\texttt{Rodrigues}\left(\texttt{rvec}\right)
\]

Then, through some simple calculations, we can get the Euler angle.

\subsection{3-point Localization}
The \texttt{apriltags}'s center point is accurate enough to calculate the position of the camera. That is to say, through at least $3$ \texttt{apriltags}, we can calculate the position of the camera through the P$n$P algorithm.

Knowing $z$, $\mathrm{roll}$ and $\mathrm{pitch}$, we should calculate the ``bias'' matrix.

Define the original matrixes of rotate and transform through these elements:

\begin{equation}
  \boldsymbol{R}_x=
  \left(
    \begin{matrix}
      1 & 0 & 0 \\
      0 & \cos\alpha & -\sin\alpha \\
      0 & \sin\alpha & \cos\alpha
    \end{matrix}
  \right),
  \boldsymbol{R}_z=
  \left(
    \begin{matrix}
      \cos\gamma & -\sin\gamma & 0 \\
      \sin\gamma & \cos\gamma & 0 \\
      0 & 0 & 1
    \end{matrix}
  \right)
\end{equation}

Then, we can get the rotate ``bias'' matrix:

\begin{equation}
  \boldsymbol{R}_{\texttt{bias}}=\boldsymbol{R}_x\cdot \boldsymbol{R}_z= \left(
    \begin{matrix}
      \cos\gamma & -\sin\gamma & 0 \\
      \sin\gamma\cos\alpha & \cos\gamma\cos\alpha & -\sin\alpha \\
      \sin\gamma\sin\alpha & \cos\gamma\sin\alpha & \cos\alpha
    \end{matrix}
  \right)
\end{equation}

Through \texttt{Rodrigues} method, we can get the rotate vector of the ``bias'' matrix.

Also, the transform vector is easy to get:

\begin{equation}
  \boldsymbol{t}_{\texttt{bias}}=\left(
    \begin{matrix}
      0 \\
      0 \\
      -z
    \end{matrix}
  \right)
\end{equation}

\subsection{1-point Emergency Localization}
We can know each corner of the \texttt{apriltags} of the map. Therefore, we can calculate the position of the camera through the P$n$P algorithm.

However, I strongly recommend that we should not use this method. It is not so accurate and it is easy to make mistakes.

If there is less than $3$ \texttt{apriltags} in the view of the camera, we can calculate corners of the \texttt{apriltags} in the image. Then we can calculate the position of the camera through the P$n$P algorithm.

\subsection{Turn Around}
The robot can never gonna give you up, never gonna let you down. It can never gonna run around and desert you.

So, don't cry, don't say goodbye, don't tell a lie and hurt you.

If there's no \texttt{apriltags}, what the robot should do is to turn around and find the \texttt{apriltags}. There is no place without \texttt{apriltag} around the robot.

\subsection{Block Recognition}

\subsubsection{Color Recognition}
Through the \texttt{HSV} color space, we can easily recognize the color of the block. The \texttt{HSV} color space is a cylindrical coordinate system. The three dimensions represent the hue, saturation, and value, respectively.

We use the opening operation to remove the noise. That's becuase there are white dots on colored blocks and it is not so easy to remove. The opening operation can relatively remove the white dots, making the color recognition more accurate.

The adjustment of target range is a hard work. We should adjust the range of the color recognition to make it more accurate.

\subsubsection{Shape Recognition}
The camera can only capture the two-dimensional image. So we simply use the \texttt{N2} algorithm to recognize the shape of the block:

\[
  \left(\begin{matrix}x&y\end{matrix}\right)=\left(\begin{matrix}\sqrt{\dfrac{\sum x_i^2}{n}}&\sqrt{\dfrac{\sum y_i^2}{n}}\end{matrix}\right)
\]

The reason why the arithmetic mean is not used is because I think it is not elegant enough.
\end{document}
